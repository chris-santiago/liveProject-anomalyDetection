{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Lato-Bold;\f1\fnil\fcharset0 Lato-Regular;\f2\froman\fcharset0 Times-Roman;
\f3\fnil\fcharset0 Lato-Italic;\f4\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red241\green241\blue241;
\red18\green141\blue221;\red184\green14\blue61;\red247\green238\blue241;\red38\green38\blue38;\red242\green242\blue242;
\red49\green49\blue49;\red5\green48\blue77;\red255\green255\blue231;\red172\green199\blue237;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\c83922;\cssrgb\c100000\c100000\c100000;\cssrgb\c95686\c95686\c95686;
\cssrgb\c392\c62745\c89412;\cssrgb\c78039\c14510\c30588;\cssrgb\c97647\c94902\c95686;\cssrgb\c20000\c20000\c20000;\cssrgb\c96078\c96078\c96078;
\cssrgb\c25098\c25098\c25098;\cssrgb\c0\c25098\c37647;\cssrgb\c100000\c100000\c92549;\cssrgb\c72941\c82353\c94510;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid102\'02\'01.;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat9\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid201\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat14\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid301\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid401\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl1120\sa600\partightenfactor0

\f0\b\fs112 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Training an Anomaly Detection Model\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\fs42 \cf2 \cb4 Objective
\f1\b0 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls1\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Train an anomaly detection model using the isolation forest model. We will also export the model and visualize its decision boundary.\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b \cf2 \cb4 About isolation forest
\f1\b0 \cb1 \
\pard\pardeftab720\sl600\partightenfactor0
\cf2 \cb4 An \'93{\field{\*\fldinst{HYPERLINK "https://www.researchgate.net/publication/224384174_Isolation_Forest"}}{\fldrslt 
\f2 \cf5 \strokec5 Isolation Forest}}\'94 (Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou, 2008) is an unsupervised learning algorithm for identifying anomalies\'97data observations that follow a different pattern than normal instances. Unlike most anomaly detection algorithms, which work by learning the normal or common patterns and classifying the rest as anomalies, an isolation forest focuses on isolating the anomalies and not the normal cases. According to the authors, the drawback of the first approach is that these methods are optimized for profiling the normal instances and not the anomalies and their constraint to perform well in only low-dimensional data. On the other hand, isolation forests focus on the property that anomalies are usually the minority class among a dataset and that their features differ from the normal ones. As a result, and quoting the paper, \'93anomalies are \'91few and different,\'92 which make them more susceptible to isolation than normal points.\'94\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b \cf2 \cb4 Workflow
\f1\b0 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls2\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Picking up where we last left off (the\'a0
\f3\i liveProject/jupyter/
\f1\i0 \'a0directory), add\'a0
\f0\b scikit-learn
\f1\b0 \'a0and\'a0
\f0\b matplotlib
\f1\b0 \'a0to our list of requirements in\'a0
\f3\i requirements.txt
\f1\i0 .\cb1 \uc0\u8232 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Build the Docker image.\cb1 \uc0\u8232 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Start the Docker image as we did in Milestone 2, step 5, and access Jupyter. From Jupyter, create a new notebook and name it\'a0
\f3\i train
\f1\i0 .\cb1 \uc0\u8232 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Our tasks in this Milestone are loading the dataset, training the model, and visualizing its decision boundary. Therefore, in the notebook\'92s first cell, import pandas, NumPy, Matplotlib, and scikit-learn\'92s isolation forest class.\cb1 \uc0\u8232 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Load both the training and test dataset.\cb1 \uc0\u8232 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Now, we will train the model.\cb1 \uc0\u8232 \cb4 a. To train it, create an instance of an\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 IsolationForest
\f1\fs42 \cf2 \cb4 \strokec2 \'a0model and use the parameter\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 random_state
\f1\fs42 \cf2 \cb4 \strokec2 \'a0set to\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 16
\f1\fs42 \cf2 \cb4 \strokec2 \'a0for reproducibility purposes. You can find the model\'92s documentation at \'93{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"}}{\fldrslt 
\f2 \cf5 \strokec5 sklearn.ensemble.IsolationForest}}.\'94 Assigning this parameter ensures we will all obtain the same results. Why 16? Just a random number; there\'92s no particular reason.\cb1 \uc0\u8232 \cb4 b. Use the method\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 model.fit()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0with the loaded training set as a parameter to train it. That\'92s it; we have the model. Wait, do we? Let\'92s take a look.\cb1 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Note
\f1\b0 : As we saw in the introduction, an isolation forest isolates the anomalous points from the non-anomalous. In layman\'92s terms, we could say that it builds a frontier that separates these anomalous points from the non-anomalous. This frontier is sometimes known as the decision boundary or decision function, and we can obtain it using as proxy the anomalies scores obtained with the method\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 model.decision_function()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0of the input samples. In the next step, we will visualize it.\cb1 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls2\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Draw the model\'92s decision function.\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl600\partightenfactor0
\ls2\ilvl1\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Drawing the boundary is not a trivial problem. Hence, here\'92s a snippet that does it (code was modified from an official scikit-learn example). But before you copy/paste, let\'92s quickly go through it and discuss it.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\partightenfactor0
\ls2\ilvl0
\f4\fs26 \cf8 \cb9 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	9.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 import matplotlib.pyplot as plt\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	10.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 \
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	11.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 # Change the plot's size.\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	12.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.rcParams['figure.figsize'] = [15, 15]\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	13.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 \
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	14.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 # Plot of the decision frontier\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	15.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 xx, yy = np.meshgrid(np.linspace(-2, 70, 100), np.linspace(-2, 70, 100))\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	16.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	17.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 Z = Z.reshape(xx.shape)\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	18.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.title("Decision Boundary (base model)")\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	19.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 # This draw the "soft" or secondary boundaries.\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	20.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 8), cmap=plt.cm.PuBu, alpha=0.5)\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	21.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 # This draw the line that separates the hard from the soft boundaries.\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	22.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	23.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 # This draw the hard boundary\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	24.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	25.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.scatter(X_train.iloc[:, 0],\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	26.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8             X_train.iloc[:, 1],\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	27.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8             edgecolors='k')\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	28.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.xlabel('Mean')\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	29.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.ylabel('SD')\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	30.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.grid(True)\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	31.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 plt.show()\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	32.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl600\partightenfactor0
\ls2\ilvl1
\f1\fs42 \cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The main idea behind the visualization involves generating many points covering the range of both features of the training set and predicting the anomaly scores of these generated points with the\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 decision_function()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0function (note that I named my model variable\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 clf
\f1\fs42 \cf2 \cb4 \strokec2 ; yours might have a different name). After predicting, we then draw the generated points using Matplotlib\'92s contour plot to see the \'93ripples\'94 (the different boundary levels). After drawing it, you will see a \'93hard\'94 boundary in red and \'93soft\'94 boundaries in different shades of blue. Make sure you also add the training set (I named mine\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 X_train
\f1\fs42 \cf2 \cb4 \strokec2 ; yours might have a different name) to the plot using\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 plt.scatter()
\f1\fs42 \cf2 \cb4 \strokec2 .\cb1 \
\ls2\ilvl1\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Copy/paste the snippet above to draw the decision boundary. The image that follows shows how the plot should look.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \strokec8 \pard\pardeftab720\partightenfactor0
\cf8 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls3\ilvl0
\fs42 \cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	9.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Let\'92s take this milestone step to interpret what we see here. The plot you have on screen is the decision boundary of the isolation forest. Everything within the red region is what I call the \'93hard\'94 decision boundary. It contains the inliers or normal points. The rest are the outliers or anomalies. Here, we can barely see the red region\'97it is extremely small compared to the rest of the space. Yet, it contains most of the points (around 85%) of the training dataset. However, the remaining 15% is still a lot of data, and probably many of these points are data points that aren\'92t that extremely anomalous or points that might be even false positives. So, for this case, it would be wise to increase the decision boundary\'92s size, thus also reducing the points that would be classified as outliers. How? By using the\'a0
\f0\b contamination
\f1\b0 \'a0hyperparameter. See note 1 in the notes section below.\cb1 \
\ls3\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	10.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Retrain the model using the\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 contamination
\f1\fs42 \cf2 \cb4 \strokec2 \'a0hyperparameter set to\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 0.001
\f1\fs42 \cf2 \cb4 \strokec2 . This hyperparameter controls the proportion of outliers in the dataset, so we will force the model to build its decision function with the constraint that only 0.1% of the data observations are outliers. With this hyperparameter, we will drastically reduce and control the outliers\'92 space.\cb1 \
\ls3\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	11.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Re-plot the decision boundary of the new model. Notice the difference? In this new model, the hard decision boundary covers more space than the previous one, leaving us with values that could be considered extremely anomalous.\cb1 \
\ls3\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	12.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use the model\'92s\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 predict()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0method to test it with the test dataset. The function returns an array with the predictions where each value is either\'a0
\f0\b -1
\f1\b0 \'a0(outlier) or\'a0
\f0\b 1
\f1\b0 \'a0(inlier).\cb1 \
\ls3\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	13.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once again, plot the decision boundary, but instead of drawing the training set, draw the test dataset and color-code the inliers with blue and the outliers with red. You could do so by merging the test dataset with the predictions produced in step 12 and using one\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 plt.scatter()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0to plot the data observations where the prediction label is\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 -1
\f1\fs42 \cf2 \cb4 \strokec2 \'a0and another where the label is\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 1
\f1\fs42 \cf2 \cb4 \strokec2 . To merge the data, you could use the pandas\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 concat()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0function like this:\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 pd.concat([X_test, pd.Series(test_predictions)], axis=1)
\f1\fs42 \cf2 \cb4 \strokec2 , where\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 test_predictions
\f1\fs42 \cf2 \cb4 \strokec2 \'a0are the predictions from step 12. If everything worked as planned, all the inliers would be inside the red region and the outliers in the blue one. The resulting plot should look similar to this one:\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \strokec8 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls4\ilvl0
\fs42 \cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	14.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Export the model to the host machine using the package\'a0
\f0\b joblib
\f1\b0 \'a0(it comes with scikit-learn\'92s installation). For details on how to do this, see the resources section.\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b \cf2 \cb4 Deliverable
\f1\b0 \cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \cb4 This milestone requires three deliverables:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls5\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The exported model in joblib format.\cb1 \
\ls5\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The training notebook.\cb1 \
\ls5\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3.	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The decision boundary plot with the test data.\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \cb4 The exported model should be named \'93model.joblib\'94 and exported using\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 joblib.dump()
\f1\fs42 \cf2 \cb4 \strokec2 , as explained in the model persistence document presented in the resources section. The training notebook needs to include all the steps indicated here; that is, the import of the data, model training, and visualizations. As for the decision boundary chart, it should have the contours plus the scatterplot with the test data.\cb1 \
\cb4 In Milestone 4, we will deploy the model here created in a web service.\cb1 \
\pard\pardeftab720\sl600\partightenfactor0
\cf2 \cb4 Upload a link to your deliverable in the Submit Your Work section and click submit. After submitting, the Author\'92s solution and peer solutions will appear on the page for you to examine.\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b \cf2 \cb4 Importance to project
\f1\b0 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls6\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In this milestone, we have created the anomaly detection model, which we could consider the center and key component of the platform. Later, we will serve the model using a web service, and generate metrics.\cb1 \
\ls6\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Besides training the model, we explored the concept of anomaly detection and one of its most popular algorithms, Isolation Forest. Regarding the algorithm itself, we quickly described it and introduced one of its most crucial hyperparameters, contamination.\cb1 \
\ls6\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 While working with the model, we also learned the concept of decision boundary and how to visualize it using the function\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 decision_function()
\f1\fs42 \cf2 \cb4 \strokec2 \'a0and Matplotlib.\cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b \cf2 \cb4 Help
\f1\b0 \cb1 \
\pard\pardeftab720\sl600\partightenfactor0
\cf2 \cb4 Feeling stuck? Use as little or as much help as you need to reach the solution!\cb1 \
\pard\pardeftab720\qr\partightenfactor0

\f2\fs32 \cf10 \cb3 \strokec10 resources\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs36 \cf11 \cb4 \strokec11 Introducing Data Science by Davy Cielen, Arno D. B. Meysman, and Mohamed Ali\cb1 \
\pard\pardeftab720\sl600\partightenfactor0

\f1\b0\fs42 \cf2 \cb12 \strokec2 Chapter 3, Unit 3.2, Unsupervised learning. Many anomaly detection algorithms fall under the category of unsupervised learning. In the following excerpt, you will find an introduction to this learning paradigm.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \strokec8 \
\pard\pardeftab720\sl600\partightenfactor0

\fs42 \cf2 \cb4 \strokec2 Isolation forest is one of the many algorithms capable of detecting anomalies. In the following excerpt, you will find two groups of anomaly detectors algorithms applied to fraud detection.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs36 \cf11 \cb4 \strokec11 Graph-Powered Machine Learning by Alessandro Negro\cb1 \
\pard\pardeftab720\sl600\partightenfactor0

\f1\b0\fs42 \cf2 \cb12 \strokec2 Chapter 9, Unit 1, Proximity-based Algorithm and Chapter 9, Unit 2, Distance-based Approach. In these sections, you will find two groups of anomaly detectors algorithms applied to fraud detection.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \strokec8 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b\fs42 \cf2 \cb4 \strokec2 Additional resources
\f1\b0 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls7\ilvl0\cf2 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 scikit-learn\'92s\'a0{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"}}{\fldrslt \cf5 \strokec5 sklearn.ensemble.IsolationForest}}.\cb1 \uc0\u8232 \
\ls7\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 scikit-learn\'92s \'93{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/model_persistence.html"}}{\fldrslt \cf5 \strokec5 Model persistence}}\'94 describes how to load, import , and export a scikit-learn model.\cb1 \uc0\u8232 \
\ls7\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \'93{\field{\*\fldinst{HYPERLINK "https://www.researchgate.net/publication/224384174_Isolation_Forest"}}{\fldrslt \cf5 \strokec5 Isolation Forest}},\'94 a paper by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.\cb1 \uc0\u8232 \
\pard\pardeftab720\qr\partightenfactor0

\f2\fs32 \cf10 \cb3 \strokec10 help\cb1 \
\
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f1\fs42 \cf2 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls8\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls9\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls10\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls11\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls12\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl600\partightenfactor0
\ls13\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl280\partightenfactor0

\f2\fs28 \cf13 \strokec13 \
\pard\pardeftab720\qr\partightenfactor0

\fs32 \cf10 \cb3 \strokec10 partial solution\cb1 \
\
\pard\pardeftab720\sl600\partightenfactor0

\f1\fs42 \cf2 \strokec2 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\partightenfactor0

\f4\fs26 \cf8 \cb9 \strokec8 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f1\fs42 \cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl360\partightenfactor0

\f4\fs26 \cf8 \cb9 \strokec8 \
\pard\pardeftab720\sl420\qc\partightenfactor0

\f2\fs30 \cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl280\partightenfactor0

\fs28 \cf13 \strokec13 \
\pard\pardeftab720\qr\partightenfactor0

\fs32 \cf10 \cb3 \strokec10 full solution\cb1 \
\
\pard\pardeftab720\sl600\partightenfactor0

\f1\fs42 \cf2 \strokec2 \
\pard\pardeftab720\sl420\qc\partightenfactor0

\f2\fs30 \cf3 \strokec3 \
\pard\pardeftab720\sl280\partightenfactor0

\fs28 \cf13 \strokec13 \
\pard\pardeftab720\sl600\sa600\partightenfactor0

\f0\b\fs42 \cf2 \cb4 \strokec2 About Makefile
\f1\b0 \cb1 \
\pard\pardeftab720\sl600\sa600\partightenfactor0
\cf2 \cb4 By this point, we have executed the docker run command at least three times; and that\'92s great because now we know how to use it. However, typing such a large and verbose command takes time, and it\'92s prone to errors. To streamline the process, I\'92d suggest using a\'a0
\f0\b Makefile
\f1\b0 . A Makefile is a build automation tool consisting of a file named\'a0
\f3\i Makefile
\f1\i0 \'a0(without extension). This Makefile consists of named rules that, upon executing them, execute a system command\'97like an alias. For example, we could have a rule named\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 run
\f1\fs42 \cf2 \cb4 \strokec2 \'a0whose system command is\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 docker run -v $(PWD):/src -p 8888:8888 \{your_name\}/lp-jupyter
\f1\fs42 \cf2 \cb4 \strokec2 . So instead of executing the Docker command, we could use\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 make run
\f1\fs42 \cf2 \cb4 \strokec2 . Shorter, right? The Makefile file should look as follows:\cb1 \
\pard\pardeftab720\sl360\partightenfactor0

\f4\fs26 \cf8 \cb9 \strokec8 run:\
    docker run -v $(PWD):/src -p 8888:8888 \{your_name\}/lp-jupyter\
\pard\pardeftab720\sl600\partightenfactor0

\f1\fs42 \cf2 \cb4 \strokec2 To use it, execute\'a0
\f4\fs37\fsmilli18900 \cf6 \cb7 \strokec6 make run
\f1\fs42 \cf2 \cb4 \strokec2 \'a0from the Terminal while same in the path as the Makefile.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf3 \strokec3 \
}